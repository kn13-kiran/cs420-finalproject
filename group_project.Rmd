---
title: 'Predicting House Prices'
author: ""
date: ""
output:
  html_document: 
    theme: readable
    toc: yes  
  pdf_document: default
urlcolor: cyan
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

The goal of this project is to predict house prices for King County. We will build models used for prediction of house prices based on the King County house sales dataset, which includes houses sold between May 2014 and May 2015. This dataset includes over 20k observations and the variables are information on the house sold such as square footage, year built, year renovated, price sold and so on. This dataset can be found on the [Keegle Website](https://www.kaggle.com/harlfoxem/housesalesprediction) and information on it can be found [here](https://geodacenter.github.io/data-and-lab/KingCounty-HouseSales2015/).

# Methods

```{r message=FALSE}
## Load useful libraries
library(ggplot2)
library(knitr)
library(faraway)
library(dplyr)
library(GGally)
library(MASS)
```

## Data Clean-Up

To start, we must first clean up the data set, such as by removing `NA` values and coercing certain variables to be factors, such as `waterfront`.

```{r}
## Loading data set
kc_dataset = read.csv("kc_house_data.csv")

## Inspect Data set
str(kc_dataset)

## Checking for NAs
(contains_na = kc_dataset[!complete.cases(kc_dataset),])
sum(is.na(kc_dataset))

## Changing the waterfront variable to be view and no-view instead of 1 and 0
kc_dataset$waterfront = ifelse(kc_dataset$waterfront == 0, "no-view", "view")

## Coerce variables to be factor
kc_dataset$waterfront = as.factor(kc_dataset$waterfront)
kc_dataset$zipcode = as.factor(kc_dataset$zipcode)

## Removing two variables that we consider to not be useful.
kc_dataset = subset(kc_dataset, select = -c(id, date))
```

## Data Exploration

In this section we will try to investigate insights on this dataset. Hopefully what we discover here will come in handy when building and choosing the best model. 
```{r}
summary(kc_dataset)
```
We will start by simply plotting the the house prices distribution. 

```{r}
ggplot(kc_dataset, aes(x = price)) +
geom_histogram(col = 'black', fill = 'blue', binwidth = 200000, center = 100000) +
theme_linedraw() + 
theme(plot.title = element_text(hjust = 0, face = 'bold',color = 'black'), #title settings
      plot.subtitle = element_text(face = "italic")) + #subtitle settings
labs(x = 'Price (USD)', y = 'Frequency', title = "House Sales in King County, USA",
     subtitle = "Price distribution") + #name subtitle
scale_y_continuous(labels = scales::comma, limits = c(0,8000), breaks = c(0,2000,4000,6000,8000)) + 
scale_x_continuous(labels = scales::comma) #prevent scientific number in x-axis
```

From distribution chart above, we know that price range of $200,000 - $600,000 has higher frequency than the other prices.

Now let's plot the living area, and plot area with the information of the grade and the condition shown as well. This should provide us some insights. 

```{r}
rbPal <- colorRampPalette(c('blue','green'))
rbPal2 <- colorRampPalette(c('black','red'))
colors1 <- rbPal(13)
colors2 <- rbPal2(13)

ggplot(kc_dataset, aes(x = sqft_living15, y = sqft_lot15)) + 
geom_jitter(alpha = 0.5, aes(shape = as.factor(condition), color = as.factor(grade))) +
scale_color_manual(values = colors1) +
theme_linedraw() +
theme(legend.title = element_text(size=10),
      plot.title = element_text(hjust = 0, face = 'bold',color = 'black'),
      plot.subtitle = element_text(face = "italic")) +
labs(x = 'Living Area (sq.ft)', y = 'Lot Area (sq.ft)', title = "House Sales in King County, USA",
     subtitle = "House built in 1900 - 2015") +
guides(color = guide_legend(title = "Grade"),
       shape = guide_legend(title = 'Condition')) +
scale_x_continuous(labels = scales::comma) +
scale_y_continuous(labels = scales::comma)
```
We can see in the graph above that grade above 8 is more common. Although it is hard to tell, it seems like condition of 3 is the most common.

For a better sense of the distribution of some of the numeric variables, we looked at histograms for each of them.

```{r}
par(mfrow = c(3, 6))
hist(kc_dataset$bedrooms, breaks = 20, main = "bedrooms", border="darkorange", col="dodgerblue")
hist(kc_dataset$bathrooms, breaks = 20, main = "bathrooms", border="darkorange", col="dodgerblue")
hist(kc_dataset$sqft_living, breaks = 20, main = "sqft_living", border="darkorange", col="dodgerblue")
hist(kc_dataset$sqft_lot, breaks = 20, main = "sqft_lot", border="darkorange", col="dodgerblue")
hist(kc_dataset$floors, breaks = 20, main = "floors", border="darkorange", col="dodgerblue")
hist(kc_dataset$view, breaks = 20, main = "view", border="darkorange", col="dodgerblue")
hist(kc_dataset$condition, breaks = 20, main = "condition", border="darkorange", col="dodgerblue")
hist(kc_dataset$grade, breaks = 20, main = "grade", border="darkorange", col="dodgerblue")
hist(kc_dataset$sqft_above, breaks = 20, main = "sqft_above", border="darkorange", col="dodgerblue")
hist(kc_dataset$sqft_basement, breaks = 20, main = "sqft_basement", border="darkorange", col="dodgerblue")
hist(kc_dataset$yr_built, breaks = 20, main = "yr_built", border="darkorange", col="dodgerblue")
hist(kc_dataset$yr_renovated, breaks = 20, main = "yr_renovated", border="darkorange", col="dodgerblue")
hist(kc_dataset$sqft_living15, breaks = 20, main = "sqft_living15", border="darkorange", col="dodgerblue")
hist(kc_dataset$sqft_lot15, breaks = 20, main = "sqft_lot15", border="darkorange", col="dodgerblue")
```
## Varible Correlation & Collinearity

```{r}
ggcorr(kc_dataset, name = "corr", label = TRUE, hjust = 1, label_size = 2.5, angle = -45, size = 3)
```

Based on this correlation map, we can see that `bathrooms`, `sqft_living`, `grade`, `sqft_above`, and `sqft_living15` are the predictor with the higher correlation to our target variable, which is `price`. We can also see some collinearity between some of the other variables, for example number of `bathroom` seems to be correlated to the `sqrt_living`, which makes sense since the bigger the house the more bathrooms are needed. This also goes for `sqft_lot` and `sqft_above`, it makes sense that the bigger the lot the bigger the house. 

One other thing that sticks out is the `sqft_lot` and `sqft_living` seems to related to `sqft_lot15` and `sqft_living15` respectively. This also makes sense, since the house size and the lot size will usually follow the neighborhood house size trend. 
We will be dealing with collinearity issues during variable selection.

### Tranformations Identification
Now, before we start modeling, we look at the pairs plots to see if any of the parameters are an obvious choice for transformations

For the purpose of being able to see the plots clearly, we do two things for the visual of the pair plot:
-Filter out the columns that have 35 or less discrete values, since they will probably not be candidates for transformations
-Focus only on numeric columns

```{r}
predictor_unique_values = 35
clean_df_numeric_only <- kc_dataset[,sapply(kc_dataset, is.numeric)]
predictor_to_remove_for_plot <- c()
removal_counter=1
for(pred in colnames(clean_df_numeric_only)) {
  k <- length(unique(clean_df_numeric_only[,pred])<predictor_unique_values)
  if(k<predictor_unique_values){
    predictor_to_remove_for_plot[removal_counter] <- pred
    removal_counter <- removal_counter+1
  }
}
# as zipcode needs to be used as a categorical variable.
predictor_to_remove_for_plot[removal_counter]<-"zipcode"
predictor_to_remove_for_plot

plot_predictors <- colnames(clean_df_numeric_only)[!(colnames(clean_df_numeric_only) %in% predictor_to_remove_for_plot)]
housing_data_clean_df_for_plot <- subset(clean_df_numeric_only, select = as.vector(eval(plot_predictors)))

ggpairs( housing_data_clean_df_for_plot, ggplot2::aes(color=I("sea green")), title = "ggpairs plot to see correlation and distribution", lower = list(continuous = wrap("smooth")), axisLabels = "show", switch = "both")
```

We make the following observations from the plot: Potential for transformations `price`,  `sqft_living` , `sqft_lot`, `sqft_basement`, `yr_built`,  `lat`, `long`

## Model Building

### Variable Selection

Let's start by building a model with the predictors we found to be the most correlated to price and compare it with a full model. 
```{r}
## Split data into train and test
set.seed(2021)
kc_trn_idx  = sample(nrow(kc_dataset), size = trunc(0.80 * nrow(kc_dataset)))
kc_train_data = kc_dataset[kc_trn_idx, ]
kc_test_data = kc_dataset[-kc_trn_idx, ]

## Fit the full model
full_model = lm(price ~., data = kc_train_data)

## Fit the model with only the good predictors we found earlier
good_preds_model = lm(price ~ bathrooms + sqft_living + grade + sqft_above + sqft_living15, data = kc_train_data)

summary(good_preds_model)$adj.r.squared

anova(good_preds_model, full_model)$"Pr(>F)"[2] < 0.10

```
At $ \alpha = 0.10 $, we reject the null hypothesis. This means we prefer the larger model with all the variables. So clearly we are missing some more predictors.

Let's check the variance inflation factor to find the highly correlated predictors. 

```{r warning=FALSE}
vif(full_model)
```

In practice, we worry more about the VIF values that are greater than 5. So we will be removing the following variables from our model `sqft_living`, `sqft_above`, `sqft_basement`, `yr_build`, and `yr_renovated`.

Let do a backwards AIC search to find what is the best combination of those predictors minus the ones we are excluding due to collinearity issue. 

```{r}
full_model = lm(price ~ . - sqft_living - sqft_above - sqft_basement - yr_built - yr_renovated  , data = kc_train_data)
selected_no_coll = step(full_model, trace = FALSE)
```

Let's also do an anova test to see which model is better the selected or the full additive model with all the predictors.

```{r}
full_add_model = lm(price ~ ., data = kc_train_data)
anova(selected_no_coll, full_add_model)[2, "Pr(>F)"] < 0.05
summary(selected_no_coll)$adj.r.squared
summary(full_add_model)$adj.r.squared
```

Okay, this is interesting. If we were to follow the anova test, we would pick the bigger model. Also, the bigger model also has a higher adjusted $R^2$. However, we know the bigger model had collinearity issues. Let's check the summary of the full model to see if collinearity is affecting the p-values of the predictors. 

```{r}
summary(full_add_model)
```

It seems to affect the some predictors such as `zipcode`.

Let's compare the cross validation RMSE for both the models. Maybe it will help us pick one.

```{r}

calc_loocv_rmse = function(model) {
    sqrt(mean((resid(model) / (1 - hatvalues(model))) ^ 2))
}

calc_loocv_rmse(selected_no_coll)
calc_loocv_rmse(full_add_model)
```
The selected model has a LOOCV-RMSE value of `r calc_loocv_rmse(selected_no_coll)` and the full additive model has a value of `r calc_loocv_rmse(full_add_model)`, which is smaller than the selected model. Thus, Considering that both the anova test, the LOOCV-RMSE test and the adjusted $R^2$ suggest that the full model is better, we will proceed with the full model. 

#### AIC Backwards Selection 

Let's try a AIC backwards selection on the full.

```{r}
selected_add_model = step(full_add_model, trace = FALSE)

```

Let's compare this model with the full model:

```{r}
anova(selected_add_model, full_model)[2, "Pr(>F)"] < 0.05
summary(selected_add_model)$adj.r.squared
summary(full_add_model)$adj.r.squared

calc_loocv_rmse(selected_add_model)
calc_loocv_rmse(full_add_model)
```
Interesting, they have the same adjusted $R^@$ and same LOOCV-RMSE values. The anova test does suggest the full model is better tho at $ \alpha = 0.05 $.

### Tranformations of Predictors

We will identify the lambda transformations for the predictors we identified during data exploration and using those variables as response, fit the model, but keep `log(price)` in the predictor with others.

```{r}
m3 <- lm("sqft_living~.-price+log(price)", data = kc_train_data)
boxcox(m3,xlab = "lambda for sqft_living")

m3 <- lm("sqft_lot~.-price+log(price)", data = kc_train_data)
boxcox(m3,xlab = "lambda for sqft_lot")

m3 <- lm("yr_built~.-price+log(price)", data = kc_train_data)
boxcox(m3,xlab = "lambda for yr_built")

```

Based on the graphs above, We should apply y^2 transformation to `sqrt_lot` and `sqrt_living`.  Let's do that:

```{r}
add_model_pred_trans = lm(price ~ . - sqft_lot - sqft_living + log(sqft_lot) + log(sqft_living), data = kc_train_data)

summary(add_model_pred_trans)$adj.r.squared
summary(full_add_model)$adj.r.squared

calc_loocv_rmse(add_model_pred_trans)
calc_loocv_rmse(full_add_model)
```

As seen above, the model with the transformations has a higher adjusted $R^2$ and a lower LOOCV-RMSE. 

### Influential Observations

Before we proceed, we must check if there are influential observations that should be ignored. 

```{r}
sum(cooks.distance(full_add_model) > 4 / length(cooks.distance(full_add_model)))
```
We can see that there are `r sum(cooks.distance(full_add_model) > 4 / length(cooks.distance(full_add_model)))` influential points in the model. 

Let's try removing them and building a new model then checking the model's performance to see if it was warranted. 

```{r}

obs_to_remove_idx = which(cooks.distance(full_add_model) > 4 / length(cooks.distance(full_add_model)))
kc_train_data_infl_rmvd = kc_train_data[-obs_to_remove_idx,]

full_add_model_infl_rmvd = lm(price ~ ., data = kc_train_data_infl_rmvd)

summary(full_add_model_infl_rmvd)$adj.r.squared
summary(full_add_model)$adj.r.squared

calc_loocv_rmse(full_add_model)

calc_loocv_rmse(full_add_model_infl_rmvd)
```

As seen above, the full new additive model without the influential observations has a adjusted $R^2$ of `r summary(full_add_model_infl_rmvd)$adj.r.squared` which is better than the full additive model which has a value of `r summary(full_add_model)$adj.r.squared`. 

The LOOCV-RMSE score for the model without influential observations, `r calc_loocv_rmse(full_add_model_infl_rmvd)` is also much lower than the full model, `r calc_loocv_rmse(full_add_model)`. So we will proceed with the data without the influential points. 

### Model Assumptions

Let's take a look at the diagnostics for this model. First we defined a helper function to help us plot the fitted vs residuals and QQ plots. 

```{r}
diagnostics = function(model, pcol = "grey", lcol = "dodgerblue", alpha = 0.05, plotit = TRUE, testit = TRUE){
  
  if(plotit) {
    plot(fitted(model), resid(model), col = pcol, pch = 20,
         xlab = "Fitted", ylab = "Residuals", main = "Fitted vs Residuals")
    abline(h = 0, col = lcol, lwd = 2)
    
    qqnorm(resid(model), main = "Normal Q-Q Plot", col = pcol)
    qqline(resid(model), col = lcol, lwd = 2)
  }
  
  if(testit){
    p_val = shapiro.test(resid(model))$p.value
    decision = ifelse(p_val < alpha, "Reject","Fail to Reject")
    list(p_val = p_val, decision = decision)
  }
}
```

Now let's check the assumptions of the full additive model.

```{r}
diagnostics(full_add_model, testit = FALSE)
```

We can see that both assumptions of equal variance and normality don't hold. 

Let's try the transforming the response. 

```{r}
full_add_log_model = lm(log(price) ~ ., data = kc_train_data_infl_rmvd)

diagnostics(full_add_log_model, testit = FALSE)
```

The graphs look much better. Transforming the response is the way to go. 

### Putting it all together

Let's create a model that both transforms the response and the selected predictors, with no influential observations. 
```{r}
add_log = lm(log(price) ~ . - sqft_lot - sqft_living + log(sqft_lot) + log(sqft_living), data = kc_train_data_infl_rmvd)

summary(add_log)$adj.r.squared
```
Let's do a AIC backwards each to see if we can decrease the number of predictors even further.

```{r}
selected_add_log = step(add_log, trace=FALSE)

summary(selected_add_log)$adj.r.squared
```
This selected model does not seem to do much better than the full log model. 

## Results

Comparing all the models we have tried so far:
```{r warning=FALSE}
table = data.frame(Model = c("Full Additive", "Add w/o Collinear Preds", "Full Add Log Res/Pred", "Selected Add Log Res/Pred"),
         Adj_R_Squared = c(summary(full_add_model)$adj.r.squared, 
                           summary(selected_no_coll)$adj.r.squared, 
                           summary(add_log)$adj.r.square,
                           summary(selected_add_log)$adj.r.square
                          ),
         
         Train_RMSE = c(sqrt(mean((kc_train_data$price - predict(full_add_model, kc_train_data)) ^ 2)),
                        sqrt(mean((kc_train_data$price - predict(selected_no_coll, kc_train_data)) ^ 2)),
                        sqrt(mean((kc_train_data_infl_rmvd$price - exp(predict(add_log, kc_train_data_infl_rmvd))) ^ 2)),
                        sqrt(mean((kc_train_data_infl_rmvd$price - exp(predict(selected_add_log, kc_train_data_infl_rmvd))) ^ 2))
                      ),
         
         Test_RMSE = c(sqrt(mean((kc_test_data$price - predict(full_add_model, kc_test_data)) ^ 2)),
                        sqrt(mean((kc_test_data$price - predict(selected_no_coll, kc_test_data)) ^ 2)),
                        sqrt(mean((kc_test_data$price - exp(predict(add_log, kc_test_data))) ^ 2)),
                        sqrt(mean((kc_test_data$price - exp(predict(selected_add_log, kc_test_data))) ^ 2))
                      ),
         
         Number_Predictors = c(length(coef(full_add_model)) - 1, 
                               length(coef(selected_no_coll)) - 1,
                               length(coef(add_log)) - 1,
                               length(coef(selected_add_log)) - 1)
         )

kable(table,caption="Model Selection Table")
```



## Discussion

Since the goal of this project is to predict house prices, we will choose the model with the least errors, which is the selected log model with the response and some predictors transformed and no influential observations. It might be harder to interpret due to the number of predictions, but it performs better than the full additive model without overfitting.  


